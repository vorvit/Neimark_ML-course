{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Классификация изображений с помощью сверточных нейронных сетей**\n\nВ данном задании Вам необходимо разработать архитектуру сверточной ИНС, обеспечивающую наибольшую точность при ограничении на количество операций (FLOPs <= 0.707e6).\nЗаготовка кода для выполнения задания приведена выше. Вашей задачей будет заполнить пропущеные места, которые отмечены ключевым словом *None*.\nНеобходимая точность (accuracy) сети на датасете CIFAR100 - 30%\nЖелаемая точность (accuracy) сети на датасете CIFAR100 - 45%","metadata":{}},{"cell_type":"code","source":"!pip install keras-flops","metadata":{"execution":{"iopub.status.busy":"2023-01-25T00:49:28.791353Z","iopub.execute_input":"2023-01-25T00:49:28.791994Z","iopub.status.idle":"2023-01-25T00:49:56.130727Z","shell.execute_reply.started":"2023-01-25T00:49:28.791876Z","shell.execute_reply":"2023-01-25T00:49:56.129292Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting keras-flops\n  Downloading keras_flops-0.1.2-py3-none-any.whl (5.3 kB)\nRequirement already satisfied: tensorflow<3.0,>=2.2 in /opt/conda/lib/python3.7/site-packages (from keras-flops) (2.6.4)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (5.0)\nCollecting h5py~=3.1.0\n  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.12.1)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.15.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.1.2)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.6.3)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.3.0)\nRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.6.0)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.12)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (3.20.3)\nRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.6.0)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.4.0)\nCollecting typing-extensions<3.11,>=3.7\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.2.0)\nRequirement already satisfied: six~=1.15.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.15.0)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.1.0)\nCollecting numpy~=1.19.2\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorboard<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (2.6.0)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (1.51.1)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow<3.0,>=2.2->keras-flops) (0.37.1)\nRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow<3.0,>=2.2->keras-flops) (1.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.28.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.35.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.3.7)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.8.1)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (59.8.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.4.6)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.2.2)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.6.1)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (4.8)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (4.13.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2022.12.7)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (1.26.13)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow<3.0,>=2.2->keras-flops) (3.2.0)\nInstalling collected packages: typing-extensions, numpy, h5py, keras-flops\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.1.1\n    Uninstalling typing_extensions-4.1.1:\n      Successfully uninstalled typing_extensions-4.1.1\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.7.0\n    Uninstalling h5py-3.7.0:\n      Successfully uninstalled h5py-3.7.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\nxarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\ntfx-bsl 1.9.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.52.0 which is incompatible.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\ntensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 3.20.3 which is incompatible.\nrich 12.6.0 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\npytorch-lightning 1.8.6 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\npytools 2022.1.12 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\npytoolconfig 1.2.4 requires typing-extensions>=4.4.0; python_version < \"3.8\", but you have typing-extensions 3.10.0.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.3 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nortools 9.5.2237 requires protobuf>=4.21.5, but you have protobuf 3.20.3 which is incompatible.\nnnabla 1.32.0 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\nnnabla 1.32.0 requires protobuf<=3.19.4; platform_system != \"Windows\", but you have protobuf 3.20.3 which is incompatible.\njaxlib 0.3.25+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\njax 0.3.25 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\nimbalanced-learn 0.10.1 requires joblib>=1.1.1, but you have joblib 1.0.1 which is incompatible.\nflax 0.6.3 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\nflake8 5.0.4 requires importlib-metadata<4.3,>=1.1.0; python_version < \"3.8\", but you have importlib-metadata 4.13.0 which is incompatible.\nfeaturetools 1.11.1 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\ncupy-cuda110 11.4.0 requires numpy<1.26,>=1.20, but you have numpy 1.19.5 which is incompatible.\ncmudict 1.0.13 requires importlib-metadata<6.0.0,>=5.1.0, but you have importlib-metadata 4.13.0 which is incompatible.\ncmdstanpy 1.0.8 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\nallennlp 2.10.1 requires h5py>=3.6.0, but you have h5py 3.1.0 which is incompatible.\nallennlp 2.10.1 requires numpy>=1.21.4, but you have numpy 1.19.5 which is incompatible.\naioitertools 0.11.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\naiobotocore 2.4.2 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.29.44 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed h5py-3.1.0 keras-flops-0.1.2 numpy-1.19.5 typing-extensions-3.10.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Импорт необходимых библиотек\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras_flops import get_flops\nfrom tensorflow import keras\nfrom keras import layers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-25T00:51:09.705832Z","iopub.execute_input":"2023-01-25T00:51:09.706213Z","iopub.status.idle":"2023-01-25T00:51:14.612170Z","shell.execute_reply.started":"2023-01-25T00:51:09.706175Z","shell.execute_reply":"2023-01-25T00:51:14.611123Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Глобальные константы\nCLASSES       = 100\nBATCH_SIZE    = 128\nLEARNING_RATE = 1e-2","metadata":{"execution":{"iopub.status.busy":"2023-01-25T00:51:19.367952Z","iopub.execute_input":"2023-01-25T00:51:19.368585Z","iopub.status.idle":"2023-01-25T00:51:19.377980Z","shell.execute_reply.started":"2023-01-25T00:51:19.368547Z","shell.execute_reply":"2023-01-25T00:51:19.374317Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Выполните загрузку модели\n(X_train, y_train), (X_val, y_val) = tf.keras.datasets.cifar100.load_data()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T00:51:24.729297Z","iopub.execute_input":"2023-01-25T00:51:24.729914Z","iopub.status.idle":"2023-01-25T00:51:28.754430Z","shell.execute_reply.started":"2023-01-25T00:51:24.729871Z","shell.execute_reply":"2023-01-25T00:51:28.753487Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n169009152/169001437 [==============================] - 2s 0us/step\n169017344/169001437 [==============================] - 2s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Преобразуйте метки классов в one_hot формат\ny_train = (y_train == np.arange(CLASSES)).astype(np.float32)\ny_val = (y_val == np.arange(CLASSES)).astype(np.float32)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T00:51:33.031984Z","iopub.execute_input":"2023-01-25T00:51:33.032342Z","iopub.status.idle":"2023-01-25T00:51:33.054798Z","shell.execute_reply.started":"2023-01-25T00:51:33.032307Z","shell.execute_reply":"2023-01-25T00:51:33.053644Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# убедитесь, что данная ячейка выполняется без ошибок\nassert X_train.shape == (50000, 32, 32, 3)\nassert X_val.shape == (10000, 32, 32, 3)\nassert y_train.shape == (50000, 100)\nassert y_val.shape == (10000, 100)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T00:51:46.968969Z","iopub.execute_input":"2023-01-25T00:51:46.969323Z","iopub.status.idle":"2023-01-25T00:51:46.975145Z","shell.execute_reply.started":"2023-01-25T00:51:46.969291Z","shell.execute_reply":"2023-01-25T00:51:46.973864Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Задайте архитектуру модели\nfrom keras.models import Model\n\ninputs = keras.Input(shape=[32,32,3])\nx = layers.Conv2D(16, (3, 3), strides=(2, 2), padding='same')(inputs)\nx = layers.MaxPooling2D(pool_size=(2, 2))(x)\nx = layers.BatchNormalization()(x)\nx = layers.Activation('selu')(x)\nx = layers.Dropout(0.01)(x)\n\nx = layers.SeparableConvolution2D(32, (3, 3), strides=(1, 1), padding='same')(x)\nx = layers.MaxPooling2D(pool_size=(2, 2))(x)\nx = layers.BatchNormalization()(x)\nx = layers.Activation('leaky_relu')(x)\n\nx = layers.SeparableConvolution2D(64, (3, 3), strides=(2, 2), padding='same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.Activation('leaky_relu')(x)\n\nx = layers.SeparableConvolution2D(128, (3, 3), strides=(1, 1), padding='same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.Activation('leaky_relu')(x)\n\nx = layers.SeparableConvolution2D(128, (3, 3), strides=(2, 2), padding='same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.Activation('leaky_relu')(x)\n\nx = layers.Conv2D(64, (1, 1), padding=\"same\")(x)\nx = layers.BatchNormalization()(x)\nx = layers.Activation(\"selu\")(x)\nx = layers.Dropout(0.2)(x)\n\nx = layers.SeparableConvolution2D(256, (3, 3), strides=(1, 1), padding='same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.Activation('leaky_relu')(x)\n\nx = layers.SeparableConvolution2D(256, (3, 3), strides=(2, 2), padding='same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.Activation('leaky_relu')(x)\n\nx = layers.Flatten()(x)\nx = layers.Activation('selu')(x)\nout = layers.Dense(CLASSES, activation='softmax')(x)\n\nmodel = Model(inputs, out)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T00:52:27.259651Z","iopub.execute_input":"2023-01-25T00:52:27.260067Z","iopub.status.idle":"2023-01-25T00:52:27.453434Z","shell.execute_reply.started":"2023-01-25T00:52:27.260033Z","shell.execute_reply":"2023-01-25T00:52:27.452395Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# вычисление количества операций\nflops = get_flops(model, batch_size=1)\nprint(f\"FLOPs: {(flops / 1e6):.4f}e6\")","metadata":{"execution":{"iopub.status.busy":"2023-01-25T00:52:36.161918Z","iopub.execute_input":"2023-01-25T00:52:36.162276Z","iopub.status.idle":"2023-01-25T00:52:36.822915Z","shell.execute_reply.started":"2023-01-25T00:52:36.162244Z","shell.execute_reply":"2023-01-25T00:52:36.821963Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nscope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\n_TFProfRoot (--/692.28k flops)\n  model_1/conv2d_2/Conv2D (221.18k/221.18k flops)\n  model_1/separable_conv2d_11/separable_conv2d (131.07k/135.68k flops)\n    model_1/separable_conv2d_11/separable_conv2d/depthwise (4.61k/4.61k flops)\n  model_1/separable_conv2d_6/separable_conv2d (65.54k/83.97k flops)\n    model_1/separable_conv2d_6/separable_conv2d/depthwise (18.43k/18.43k flops)\n  model_1/separable_conv2d_8/separable_conv2d (65.54k/70.14k flops)\n    model_1/separable_conv2d_8/separable_conv2d/depthwise (4.61k/4.61k flops)\n  model_1/dense_1/MatMul (51.20k/51.20k flops)\n  model_1/separable_conv2d_9/separable_conv2d (32.77k/35.07k flops)\n    model_1/separable_conv2d_9/separable_conv2d/depthwise (2.30k/2.30k flops)\n  model_1/separable_conv2d_10/separable_conv2d (32.77k/33.92k flops)\n    model_1/separable_conv2d_10/separable_conv2d/depthwise (1.15k/1.15k flops)\n  model_1/separable_conv2d_7/separable_conv2d (16.38k/18.69k flops)\n    model_1/separable_conv2d_7/separable_conv2d/depthwise (2.30k/2.30k flops)\n  model_1/conv2d_3/Conv2D (16.38k/16.38k flops)\n  model_1/conv2d_2/BiasAdd (4.10k/4.10k flops)\n  model_1/max_pooling2d_2/MaxPool (4.10k/4.10k flops)\n  model_1/batch_normalization_8/FusedBatchNormV3 (2.14k/2.14k flops)\n  model_1/separable_conv2d_6/BiasAdd (2.05k/2.05k flops)\n  model_1/batch_normalization_15/FusedBatchNormV3 (2.05k/2.05k flops)\n  model_1/batch_normalization_14/FusedBatchNormV3 (2.05k/2.05k flops)\n  model_1/max_pooling2d_3/MaxPool (2.05k/2.05k flops)\n  model_1/batch_normalization_11/FusedBatchNormV3 (1.79k/1.79k flops)\n  model_1/batch_normalization_9/FusedBatchNormV3 (1.22k/1.22k flops)\n  model_1/batch_normalization_12/FusedBatchNormV3 (1.02k/1.02k flops)\n  model_1/batch_normalization_10/FusedBatchNormV3 (896/896 flops)\n  model_1/batch_normalization_13/FusedBatchNormV3 (512/512 flops)\n  model_1/separable_conv2d_8/BiasAdd (512/512 flops)\n  model_1/dense_1/Softmax (500/500 flops)\n  model_1/separable_conv2d_10/BiasAdd (256/256 flops)\n  model_1/separable_conv2d_11/BiasAdd (256/256 flops)\n  model_1/separable_conv2d_7/BiasAdd (256/256 flops)\n  model_1/separable_conv2d_9/BiasAdd (128/128 flops)\n  model_1/dense_1/BiasAdd (100/100 flops)\n  model_1/conv2d_3/BiasAdd (64/64 flops)\n\n======================End of Report==========================\nFLOPs: 0.6923e6\n","output_type":"stream"},{"name":"stderr","text":"2023-01-25 00:52:36.649710: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 00:52:36.650117: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n2023-01-25 00:52:36.650283: I tensorflow/core/grappler/clusters/single_machine.cc:357] Starting new session\n2023-01-25 00:52:36.650763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 00:52:36.651189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 00:52:36.651551: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 00:52:36.651949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 00:52:36.652272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2023-01-25 00:52:36.652555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n2023-01-25 00:52:36.667097: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1137] Optimization results for grappler item: graph_to_optimize\n  function_optimizer: function_optimizer did nothing. time = 1.448ms.\n  function_optimizer: function_optimizer did nothing. time = 0.001ms.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# вывод краткой информации о модели\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-01-25T00:53:09.769831Z","iopub.execute_input":"2023-01-25T00:53:09.770201Z","iopub.status.idle":"2023-01-25T00:53:09.779656Z","shell.execute_reply.started":"2023-01-25T00:53:09.770167Z","shell.execute_reply":"2023-01-25T00:53:09.778605Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 16, 16, 16)        448       \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 8, 8, 16)          0         \n_________________________________________________________________\nbatch_normalization_8 (Batch (None, 8, 8, 16)          64        \n_________________________________________________________________\nactivation_9 (Activation)    (None, 8, 8, 16)          0         \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 8, 8, 16)          0         \n_________________________________________________________________\nseparable_conv2d_6 (Separabl (None, 8, 8, 32)          688       \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 4, 4, 32)          0         \n_________________________________________________________________\nbatch_normalization_9 (Batch (None, 4, 4, 32)          128       \n_________________________________________________________________\nactivation_10 (Activation)   (None, 4, 4, 32)          0         \n_________________________________________________________________\nseparable_conv2d_7 (Separabl (None, 2, 2, 64)          2400      \n_________________________________________________________________\nbatch_normalization_10 (Batc (None, 2, 2, 64)          256       \n_________________________________________________________________\nactivation_11 (Activation)   (None, 2, 2, 64)          0         \n_________________________________________________________________\nseparable_conv2d_8 (Separabl (None, 2, 2, 128)         8896      \n_________________________________________________________________\nbatch_normalization_11 (Batc (None, 2, 2, 128)         512       \n_________________________________________________________________\nactivation_12 (Activation)   (None, 2, 2, 128)         0         \n_________________________________________________________________\nseparable_conv2d_9 (Separabl (None, 1, 1, 128)         17664     \n_________________________________________________________________\nbatch_normalization_12 (Batc (None, 1, 1, 128)         512       \n_________________________________________________________________\nactivation_13 (Activation)   (None, 1, 1, 128)         0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 1, 1, 64)          8256      \n_________________________________________________________________\nbatch_normalization_13 (Batc (None, 1, 1, 64)          256       \n_________________________________________________________________\nactivation_14 (Activation)   (None, 1, 1, 64)          0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 1, 1, 64)          0         \n_________________________________________________________________\nseparable_conv2d_10 (Separab (None, 1, 1, 256)         17216     \n_________________________________________________________________\nbatch_normalization_14 (Batc (None, 1, 1, 256)         1024      \n_________________________________________________________________\nactivation_15 (Activation)   (None, 1, 1, 256)         0         \n_________________________________________________________________\nseparable_conv2d_11 (Separab (None, 1, 1, 256)         68096     \n_________________________________________________________________\nbatch_normalization_15 (Batc (None, 1, 1, 256)         1024      \n_________________________________________________________________\nactivation_16 (Activation)   (None, 1, 1, 256)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 256)               0         \n_________________________________________________________________\nactivation_17 (Activation)   (None, 256)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 100)               25700     \n=================================================================\nTotal params: 153,140\nTrainable params: 151,252\nNon-trainable params: 1,888\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# параметры данной ячейки могут быть изменены для получения более высокой точности\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(#learning_rate = 0.01\n        learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n            0.006,\n            decay_steps=1500, # One epoch 1563 0.95\n            decay_rate=0.95,\n            staircase=True)\n    ),\n    loss=tf.keras.losses.CategoricalCrossentropy(),\n    metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-01-25T00:53:40.756774Z","iopub.execute_input":"2023-01-25T00:53:40.757139Z","iopub.status.idle":"2023-01-25T00:53:40.768503Z","shell.execute_reply.started":"2023-01-25T00:53:40.757107Z","shell.execute_reply":"2023-01-25T00:53:40.767511Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# обучения модели\nmodel.fit(\n    x=X_train,\n    y=y_train,\n    validation_data=(X_val, y_val),\n    batch_size=BATCH_SIZE,\n    callbacks=[\n        tf.keras.callbacks.ModelCheckpoint(filepath=\"{epoch:02d}-{val_accuracy:.2f}.hdf5\", save_best_only=True),\n        \n    ],\n    use_multiprocessing=True,\n    workers=8,\n    epochs=256\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-25T00:54:07.427220Z","iopub.execute_input":"2023-01-25T00:54:07.427720Z","iopub.status.idle":"2023-01-25T01:06:30.441286Z","shell.execute_reply.started":"2023-01-25T00:54:07.427676Z","shell.execute_reply":"2023-01-25T01:06:30.440352Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"2023-01-25 00:54:08.375297: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/256\n","output_type":"stream"},{"name":"stderr","text":"2023-01-25 00:54:10.481775: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"391/391 [==============================] - 11s 8ms/step - loss: 3.7772 - accuracy: 0.1116 - val_loss: 4.0888 - val_accuracy: 0.0874\nEpoch 2/256\n391/391 [==============================] - 3s 9ms/step - loss: 3.3530 - accuracy: 0.1763 - val_loss: 3.7763 - val_accuracy: 0.1695\nEpoch 3/256\n391/391 [==============================] - 3s 7ms/step - loss: 3.1605 - accuracy: 0.2135 - val_loss: 3.5699 - val_accuracy: 0.1963\nEpoch 4/256\n391/391 [==============================] - 3s 7ms/step - loss: 3.0359 - accuracy: 0.2353 - val_loss: 3.1858 - val_accuracy: 0.2301\nEpoch 5/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.9417 - accuracy: 0.2571 - val_loss: 3.3711 - val_accuracy: 0.2084\nEpoch 6/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.8680 - accuracy: 0.2702 - val_loss: 2.9630 - val_accuracy: 0.2690\nEpoch 7/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.8076 - accuracy: 0.2811 - val_loss: 3.3343 - val_accuracy: 0.2288\nEpoch 8/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.7377 - accuracy: 0.2946 - val_loss: 3.2438 - val_accuracy: 0.2303\nEpoch 9/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.6793 - accuracy: 0.3088 - val_loss: 2.8512 - val_accuracy: 0.2942\nEpoch 10/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.6338 - accuracy: 0.3206 - val_loss: 3.2270 - val_accuracy: 0.2479\nEpoch 11/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.5953 - accuracy: 0.3244 - val_loss: 2.7443 - val_accuracy: 0.3117\nEpoch 12/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.5586 - accuracy: 0.3323 - val_loss: 2.8095 - val_accuracy: 0.3048\nEpoch 13/256\n391/391 [==============================] - 3s 9ms/step - loss: 2.5122 - accuracy: 0.3446 - val_loss: 2.8908 - val_accuracy: 0.2906\nEpoch 14/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.4929 - accuracy: 0.3465 - val_loss: 2.6982 - val_accuracy: 0.3187\nEpoch 15/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.4605 - accuracy: 0.3532 - val_loss: 2.7958 - val_accuracy: 0.3016\nEpoch 16/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.4294 - accuracy: 0.3598 - val_loss: 2.6150 - val_accuracy: 0.3311\nEpoch 17/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.4016 - accuracy: 0.3631 - val_loss: 2.6214 - val_accuracy: 0.3332\nEpoch 18/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.3778 - accuracy: 0.3697 - val_loss: 2.8420 - val_accuracy: 0.3020\nEpoch 19/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.3622 - accuracy: 0.3752 - val_loss: 2.9195 - val_accuracy: 0.2946\nEpoch 20/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.3335 - accuracy: 0.3791 - val_loss: 2.6019 - val_accuracy: 0.3378\nEpoch 21/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.3057 - accuracy: 0.3856 - val_loss: 2.5842 - val_accuracy: 0.3434\nEpoch 22/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.2888 - accuracy: 0.3892 - val_loss: 2.6624 - val_accuracy: 0.3336\nEpoch 23/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.2777 - accuracy: 0.3892 - val_loss: 2.5880 - val_accuracy: 0.3386\nEpoch 24/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.2489 - accuracy: 0.3986 - val_loss: 2.7738 - val_accuracy: 0.3148\nEpoch 25/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.2375 - accuracy: 0.4011 - val_loss: 2.5296 - val_accuracy: 0.3499\nEpoch 26/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.2222 - accuracy: 0.4035 - val_loss: 2.5511 - val_accuracy: 0.3528\nEpoch 27/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.2061 - accuracy: 0.4047 - val_loss: 2.4862 - val_accuracy: 0.3633\nEpoch 28/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.1802 - accuracy: 0.4131 - val_loss: 2.7093 - val_accuracy: 0.3311\nEpoch 29/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.1668 - accuracy: 0.4164 - val_loss: 2.4947 - val_accuracy: 0.3618\nEpoch 30/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.1550 - accuracy: 0.4179 - val_loss: 2.4965 - val_accuracy: 0.3625\nEpoch 31/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.1411 - accuracy: 0.4215 - val_loss: 2.7409 - val_accuracy: 0.3256\nEpoch 32/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.1225 - accuracy: 0.4243 - val_loss: 2.5915 - val_accuracy: 0.3451\nEpoch 33/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.1091 - accuracy: 0.4288 - val_loss: 2.7151 - val_accuracy: 0.3315\nEpoch 34/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.1004 - accuracy: 0.4284 - val_loss: 2.4816 - val_accuracy: 0.3710\nEpoch 35/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.0866 - accuracy: 0.4311 - val_loss: 2.4872 - val_accuracy: 0.3685\nEpoch 36/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.0706 - accuracy: 0.4339 - val_loss: 2.4846 - val_accuracy: 0.3701\nEpoch 37/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.0576 - accuracy: 0.4377 - val_loss: 2.6490 - val_accuracy: 0.3428\nEpoch 38/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.0486 - accuracy: 0.4398 - val_loss: 2.4900 - val_accuracy: 0.3706\nEpoch 39/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.0385 - accuracy: 0.4428 - val_loss: 2.4922 - val_accuracy: 0.3688\nEpoch 40/256\n391/391 [==============================] - 3s 8ms/step - loss: 2.0225 - accuracy: 0.4456 - val_loss: 2.6286 - val_accuracy: 0.3506\nEpoch 41/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.0185 - accuracy: 0.4467 - val_loss: 2.6130 - val_accuracy: 0.3525\nEpoch 42/256\n391/391 [==============================] - 3s 7ms/step - loss: 2.0048 - accuracy: 0.4494 - val_loss: 2.4802 - val_accuracy: 0.3754\nEpoch 43/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.9864 - accuracy: 0.4542 - val_loss: 2.5372 - val_accuracy: 0.3595\nEpoch 44/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9804 - accuracy: 0.4555 - val_loss: 2.4700 - val_accuracy: 0.3778\nEpoch 45/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9725 - accuracy: 0.4566 - val_loss: 2.4588 - val_accuracy: 0.3737\nEpoch 46/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.9550 - accuracy: 0.4594 - val_loss: 2.4932 - val_accuracy: 0.3750\nEpoch 47/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9455 - accuracy: 0.4625 - val_loss: 2.4616 - val_accuracy: 0.3743\nEpoch 48/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.9323 - accuracy: 0.4629 - val_loss: 2.5051 - val_accuracy: 0.3734\nEpoch 49/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.9325 - accuracy: 0.4648 - val_loss: 2.5030 - val_accuracy: 0.3751\nEpoch 50/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.9205 - accuracy: 0.4663 - val_loss: 2.4937 - val_accuracy: 0.3787\nEpoch 51/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.9072 - accuracy: 0.4702 - val_loss: 2.5319 - val_accuracy: 0.3739\nEpoch 52/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.9018 - accuracy: 0.4713 - val_loss: 2.4894 - val_accuracy: 0.3762\nEpoch 53/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.8908 - accuracy: 0.4746 - val_loss: 2.5409 - val_accuracy: 0.3693\nEpoch 54/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.8835 - accuracy: 0.4784 - val_loss: 2.4991 - val_accuracy: 0.3802\nEpoch 55/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.8748 - accuracy: 0.4745 - val_loss: 2.4742 - val_accuracy: 0.3836\nEpoch 56/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.8650 - accuracy: 0.4793 - val_loss: 2.5173 - val_accuracy: 0.3763\nEpoch 57/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.8626 - accuracy: 0.4808 - val_loss: 2.5912 - val_accuracy: 0.3699\nEpoch 58/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.8608 - accuracy: 0.4812 - val_loss: 2.5054 - val_accuracy: 0.3774\nEpoch 59/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.8426 - accuracy: 0.4849 - val_loss: 2.5259 - val_accuracy: 0.3752\nEpoch 60/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.8327 - accuracy: 0.4862 - val_loss: 2.5071 - val_accuracy: 0.3766\nEpoch 61/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.8350 - accuracy: 0.4865 - val_loss: 2.5142 - val_accuracy: 0.3779\nEpoch 62/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.8191 - accuracy: 0.4906 - val_loss: 2.5232 - val_accuracy: 0.3794\nEpoch 63/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.8179 - accuracy: 0.4906 - val_loss: 2.5703 - val_accuracy: 0.3710\nEpoch 64/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.8084 - accuracy: 0.4901 - val_loss: 2.5501 - val_accuracy: 0.3708\nEpoch 65/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.8024 - accuracy: 0.4918 - val_loss: 2.5194 - val_accuracy: 0.3799\nEpoch 66/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.7905 - accuracy: 0.4952 - val_loss: 2.4869 - val_accuracy: 0.3897\nEpoch 67/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.7841 - accuracy: 0.4969 - val_loss: 2.4957 - val_accuracy: 0.3816\nEpoch 68/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.7738 - accuracy: 0.5009 - val_loss: 2.5100 - val_accuracy: 0.3788\nEpoch 69/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.7702 - accuracy: 0.5012 - val_loss: 2.5399 - val_accuracy: 0.3814\nEpoch 70/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.7659 - accuracy: 0.5026 - val_loss: 2.5027 - val_accuracy: 0.3859\nEpoch 71/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.7587 - accuracy: 0.5027 - val_loss: 2.5282 - val_accuracy: 0.3824\nEpoch 72/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.7585 - accuracy: 0.5043 - val_loss: 2.5444 - val_accuracy: 0.3824\nEpoch 73/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.7501 - accuracy: 0.5051 - val_loss: 2.4950 - val_accuracy: 0.3815\nEpoch 74/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.7432 - accuracy: 0.5054 - val_loss: 2.5300 - val_accuracy: 0.3782\nEpoch 75/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.7344 - accuracy: 0.5089 - val_loss: 2.6355 - val_accuracy: 0.3674\nEpoch 76/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.7311 - accuracy: 0.5113 - val_loss: 2.4940 - val_accuracy: 0.3871\nEpoch 77/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.7245 - accuracy: 0.5138 - val_loss: 2.5094 - val_accuracy: 0.3835\nEpoch 78/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.7145 - accuracy: 0.5120 - val_loss: 2.6308 - val_accuracy: 0.3716\nEpoch 79/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.7095 - accuracy: 0.5149 - val_loss: 2.5311 - val_accuracy: 0.3824\nEpoch 80/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.7083 - accuracy: 0.5141 - val_loss: 2.5046 - val_accuracy: 0.3848\nEpoch 81/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.6958 - accuracy: 0.5193 - val_loss: 2.5551 - val_accuracy: 0.3779\nEpoch 82/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.6880 - accuracy: 0.5211 - val_loss: 2.5167 - val_accuracy: 0.3868\nEpoch 83/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.6925 - accuracy: 0.5191 - val_loss: 2.5215 - val_accuracy: 0.3883\nEpoch 84/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.6801 - accuracy: 0.5211 - val_loss: 2.5410 - val_accuracy: 0.3809\nEpoch 85/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.6730 - accuracy: 0.5220 - val_loss: 2.5931 - val_accuracy: 0.3801\nEpoch 86/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.6720 - accuracy: 0.5225 - val_loss: 2.5286 - val_accuracy: 0.3816\nEpoch 87/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.6709 - accuracy: 0.5252 - val_loss: 2.5289 - val_accuracy: 0.3855\nEpoch 88/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.6678 - accuracy: 0.5265 - val_loss: 2.5891 - val_accuracy: 0.3777\nEpoch 89/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.6563 - accuracy: 0.5295 - val_loss: 2.5382 - val_accuracy: 0.3883\nEpoch 90/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.6504 - accuracy: 0.5284 - val_loss: 2.5401 - val_accuracy: 0.3816\nEpoch 91/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.6383 - accuracy: 0.5299 - val_loss: 2.5624 - val_accuracy: 0.3807\nEpoch 92/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.6549 - accuracy: 0.5261 - val_loss: 2.5339 - val_accuracy: 0.3881\nEpoch 93/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.6356 - accuracy: 0.5315 - val_loss: 2.5579 - val_accuracy: 0.3831\nEpoch 94/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.6360 - accuracy: 0.5319 - val_loss: 2.6172 - val_accuracy: 0.3748\nEpoch 95/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.6291 - accuracy: 0.5325 - val_loss: 2.6171 - val_accuracy: 0.3795\nEpoch 96/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.6319 - accuracy: 0.5309 - val_loss: 2.5502 - val_accuracy: 0.3886\nEpoch 97/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.6209 - accuracy: 0.5374 - val_loss: 2.5763 - val_accuracy: 0.3848\nEpoch 98/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.6125 - accuracy: 0.5352 - val_loss: 2.6220 - val_accuracy: 0.3747\nEpoch 99/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.6118 - accuracy: 0.5370 - val_loss: 2.5762 - val_accuracy: 0.3827\nEpoch 100/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.6102 - accuracy: 0.5391 - val_loss: 2.5491 - val_accuracy: 0.3891\nEpoch 101/256\n391/391 [==============================] - 3s 9ms/step - loss: 1.6017 - accuracy: 0.5385 - val_loss: 2.6049 - val_accuracy: 0.3807\nEpoch 102/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5953 - accuracy: 0.5397 - val_loss: 2.5630 - val_accuracy: 0.3848\nEpoch 103/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5956 - accuracy: 0.5408 - val_loss: 2.5591 - val_accuracy: 0.3918\nEpoch 104/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5907 - accuracy: 0.5433 - val_loss: 2.5941 - val_accuracy: 0.3880\nEpoch 105/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.5822 - accuracy: 0.5431 - val_loss: 2.5720 - val_accuracy: 0.3846\nEpoch 106/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5899 - accuracy: 0.5422 - val_loss: 2.6084 - val_accuracy: 0.3840\nEpoch 107/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5787 - accuracy: 0.5449 - val_loss: 2.5900 - val_accuracy: 0.3836\nEpoch 108/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5755 - accuracy: 0.5439 - val_loss: 2.6120 - val_accuracy: 0.3784\nEpoch 109/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.5717 - accuracy: 0.5469 - val_loss: 2.6170 - val_accuracy: 0.3786\nEpoch 110/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5726 - accuracy: 0.5441 - val_loss: 2.6755 - val_accuracy: 0.3729\nEpoch 111/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5714 - accuracy: 0.5446 - val_loss: 2.6361 - val_accuracy: 0.3814\nEpoch 112/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.5531 - accuracy: 0.5495 - val_loss: 2.6003 - val_accuracy: 0.3827\nEpoch 113/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.5618 - accuracy: 0.5463 - val_loss: 2.5966 - val_accuracy: 0.3873\nEpoch 114/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5560 - accuracy: 0.5494 - val_loss: 2.5822 - val_accuracy: 0.3884\nEpoch 115/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5531 - accuracy: 0.5500 - val_loss: 2.5919 - val_accuracy: 0.3882\nEpoch 116/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5517 - accuracy: 0.5508 - val_loss: 2.6094 - val_accuracy: 0.3803\nEpoch 117/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.5484 - accuracy: 0.5530 - val_loss: 2.5959 - val_accuracy: 0.3860\nEpoch 118/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5412 - accuracy: 0.5548 - val_loss: 2.6166 - val_accuracy: 0.3799\nEpoch 119/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5348 - accuracy: 0.5553 - val_loss: 2.6154 - val_accuracy: 0.3827\nEpoch 120/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5357 - accuracy: 0.5538 - val_loss: 2.5986 - val_accuracy: 0.3839\nEpoch 121/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.5289 - accuracy: 0.5570 - val_loss: 2.6141 - val_accuracy: 0.3845\nEpoch 122/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5299 - accuracy: 0.5550 - val_loss: 2.6232 - val_accuracy: 0.3847\nEpoch 123/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.5251 - accuracy: 0.5572 - val_loss: 2.6172 - val_accuracy: 0.3835\nEpoch 124/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5338 - accuracy: 0.5540 - val_loss: 2.6102 - val_accuracy: 0.3879\nEpoch 125/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5160 - accuracy: 0.5599 - val_loss: 2.6307 - val_accuracy: 0.3835\nEpoch 126/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5091 - accuracy: 0.5611 - val_loss: 2.6308 - val_accuracy: 0.3876\nEpoch 127/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5170 - accuracy: 0.5589 - val_loss: 2.6216 - val_accuracy: 0.3857\nEpoch 128/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.5113 - accuracy: 0.5584 - val_loss: 2.6095 - val_accuracy: 0.3874\nEpoch 129/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5085 - accuracy: 0.5617 - val_loss: 2.6183 - val_accuracy: 0.3853\nEpoch 130/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5104 - accuracy: 0.5596 - val_loss: 2.6481 - val_accuracy: 0.3853\nEpoch 131/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.5063 - accuracy: 0.5594 - val_loss: 2.6262 - val_accuracy: 0.3852\nEpoch 132/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.4919 - accuracy: 0.5647 - val_loss: 2.6217 - val_accuracy: 0.3851\nEpoch 133/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4959 - accuracy: 0.5639 - val_loss: 2.6394 - val_accuracy: 0.3831\nEpoch 134/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.5127 - accuracy: 0.5579 - val_loss: 2.6192 - val_accuracy: 0.3878\nEpoch 135/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4869 - accuracy: 0.5673 - val_loss: 2.6567 - val_accuracy: 0.3848\nEpoch 136/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.4878 - accuracy: 0.5658 - val_loss: 2.7024 - val_accuracy: 0.3790\nEpoch 137/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4850 - accuracy: 0.5643 - val_loss: 2.6313 - val_accuracy: 0.3826\nEpoch 138/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4877 - accuracy: 0.5674 - val_loss: 2.6444 - val_accuracy: 0.3852\nEpoch 139/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4812 - accuracy: 0.5668 - val_loss: 2.6431 - val_accuracy: 0.3840\nEpoch 140/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.4756 - accuracy: 0.5694 - val_loss: 2.6748 - val_accuracy: 0.3833\nEpoch 141/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4875 - accuracy: 0.5684 - val_loss: 2.6417 - val_accuracy: 0.3820\nEpoch 142/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4708 - accuracy: 0.5699 - val_loss: 2.6654 - val_accuracy: 0.3850\nEpoch 143/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4672 - accuracy: 0.5694 - val_loss: 2.6523 - val_accuracy: 0.3811\nEpoch 144/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.4734 - accuracy: 0.5688 - val_loss: 2.6553 - val_accuracy: 0.3849\nEpoch 145/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.4664 - accuracy: 0.5697 - val_loss: 2.6535 - val_accuracy: 0.3844\nEpoch 146/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4666 - accuracy: 0.5708 - val_loss: 2.6438 - val_accuracy: 0.3859\nEpoch 147/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4663 - accuracy: 0.5725 - val_loss: 2.6591 - val_accuracy: 0.3857\nEpoch 148/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.4673 - accuracy: 0.5686 - val_loss: 2.6509 - val_accuracy: 0.3846\nEpoch 149/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4607 - accuracy: 0.5706 - val_loss: 2.6813 - val_accuracy: 0.3850\nEpoch 150/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4662 - accuracy: 0.5683 - val_loss: 2.6627 - val_accuracy: 0.3832\nEpoch 151/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4527 - accuracy: 0.5732 - val_loss: 2.6728 - val_accuracy: 0.3843\nEpoch 152/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.4572 - accuracy: 0.5733 - val_loss: 2.6536 - val_accuracy: 0.3819\nEpoch 153/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4524 - accuracy: 0.5719 - val_loss: 2.6713 - val_accuracy: 0.3846\nEpoch 154/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4489 - accuracy: 0.5742 - val_loss: 2.6599 - val_accuracy: 0.3865\nEpoch 155/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4477 - accuracy: 0.5753 - val_loss: 2.6858 - val_accuracy: 0.3830\nEpoch 156/256\n391/391 [==============================] - 3s 9ms/step - loss: 1.4488 - accuracy: 0.5744 - val_loss: 2.6867 - val_accuracy: 0.3830\nEpoch 157/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4418 - accuracy: 0.5758 - val_loss: 2.6859 - val_accuracy: 0.3803\nEpoch 158/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4380 - accuracy: 0.5771 - val_loss: 2.6859 - val_accuracy: 0.3826\nEpoch 159/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.4347 - accuracy: 0.5783 - val_loss: 2.6586 - val_accuracy: 0.3849\nEpoch 160/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4384 - accuracy: 0.5773 - val_loss: 2.7019 - val_accuracy: 0.3800\nEpoch 161/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4336 - accuracy: 0.5793 - val_loss: 2.6691 - val_accuracy: 0.3850\nEpoch 162/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4307 - accuracy: 0.5782 - val_loss: 2.6655 - val_accuracy: 0.3860\nEpoch 163/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.4305 - accuracy: 0.5790 - val_loss: 2.6753 - val_accuracy: 0.3842\nEpoch 164/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4332 - accuracy: 0.5790 - val_loss: 2.6694 - val_accuracy: 0.3865\nEpoch 165/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4267 - accuracy: 0.5795 - val_loss: 2.6773 - val_accuracy: 0.3849\nEpoch 166/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4211 - accuracy: 0.5803 - val_loss: 2.7024 - val_accuracy: 0.3848\nEpoch 167/256\n391/391 [==============================] - 3s 9ms/step - loss: 1.4232 - accuracy: 0.5806 - val_loss: 2.6959 - val_accuracy: 0.3834\nEpoch 168/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4284 - accuracy: 0.5812 - val_loss: 2.6979 - val_accuracy: 0.3833\nEpoch 169/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4170 - accuracy: 0.5821 - val_loss: 2.6851 - val_accuracy: 0.3881\nEpoch 170/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4153 - accuracy: 0.5803 - val_loss: 2.6886 - val_accuracy: 0.3823\nEpoch 171/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.4252 - accuracy: 0.5808 - val_loss: 2.6783 - val_accuracy: 0.3827\nEpoch 172/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4180 - accuracy: 0.5835 - val_loss: 2.6890 - val_accuracy: 0.3851\nEpoch 173/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4143 - accuracy: 0.5824 - val_loss: 2.6905 - val_accuracy: 0.3825\nEpoch 174/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4201 - accuracy: 0.5835 - val_loss: 2.6866 - val_accuracy: 0.3857\nEpoch 175/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.4066 - accuracy: 0.5857 - val_loss: 2.7072 - val_accuracy: 0.3835\nEpoch 176/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4176 - accuracy: 0.5818 - val_loss: 2.7237 - val_accuracy: 0.3826\nEpoch 177/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4078 - accuracy: 0.5822 - val_loss: 2.6898 - val_accuracy: 0.3875\nEpoch 178/256\n391/391 [==============================] - 3s 9ms/step - loss: 1.4064 - accuracy: 0.5823 - val_loss: 2.7471 - val_accuracy: 0.3807\nEpoch 179/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.4105 - accuracy: 0.5818 - val_loss: 2.6855 - val_accuracy: 0.3863\nEpoch 180/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3991 - accuracy: 0.5865 - val_loss: 2.7016 - val_accuracy: 0.3836\nEpoch 181/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3977 - accuracy: 0.5857 - val_loss: 2.7044 - val_accuracy: 0.3849\nEpoch 182/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.4003 - accuracy: 0.5843 - val_loss: 2.7038 - val_accuracy: 0.3871\nEpoch 183/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4028 - accuracy: 0.5852 - val_loss: 2.7101 - val_accuracy: 0.3828\nEpoch 184/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3967 - accuracy: 0.5859 - val_loss: 2.6990 - val_accuracy: 0.3845\nEpoch 185/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.4023 - accuracy: 0.5848 - val_loss: 2.7320 - val_accuracy: 0.3844\nEpoch 186/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3924 - accuracy: 0.5878 - val_loss: 2.7092 - val_accuracy: 0.3867\nEpoch 187/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3997 - accuracy: 0.5876 - val_loss: 2.7143 - val_accuracy: 0.3843\nEpoch 188/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3984 - accuracy: 0.5858 - val_loss: 2.7026 - val_accuracy: 0.3819\nEpoch 189/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3896 - accuracy: 0.5901 - val_loss: 2.7086 - val_accuracy: 0.3818\nEpoch 190/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3927 - accuracy: 0.5852 - val_loss: 2.7474 - val_accuracy: 0.3841\nEpoch 191/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3924 - accuracy: 0.5852 - val_loss: 2.7149 - val_accuracy: 0.3849\nEpoch 192/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3926 - accuracy: 0.5878 - val_loss: 2.6986 - val_accuracy: 0.3851\nEpoch 193/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3893 - accuracy: 0.5891 - val_loss: 2.7134 - val_accuracy: 0.3843\nEpoch 194/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3873 - accuracy: 0.5897 - val_loss: 2.7200 - val_accuracy: 0.3857\nEpoch 195/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3832 - accuracy: 0.5916 - val_loss: 2.7186 - val_accuracy: 0.3855\nEpoch 196/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3833 - accuracy: 0.5910 - val_loss: 2.7254 - val_accuracy: 0.3869\nEpoch 197/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3780 - accuracy: 0.5911 - val_loss: 2.7062 - val_accuracy: 0.3856\nEpoch 198/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3843 - accuracy: 0.5885 - val_loss: 2.7189 - val_accuracy: 0.3845\nEpoch 199/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3820 - accuracy: 0.5915 - val_loss: 2.7126 - val_accuracy: 0.3856\nEpoch 200/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3788 - accuracy: 0.5894 - val_loss: 2.7196 - val_accuracy: 0.3847\nEpoch 201/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3761 - accuracy: 0.5917 - val_loss: 2.7137 - val_accuracy: 0.3850\nEpoch 202/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3753 - accuracy: 0.5921 - val_loss: 2.7145 - val_accuracy: 0.3843\nEpoch 203/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3732 - accuracy: 0.5938 - val_loss: 2.7241 - val_accuracy: 0.3840\nEpoch 204/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3736 - accuracy: 0.5944 - val_loss: 2.7255 - val_accuracy: 0.3866\nEpoch 205/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3761 - accuracy: 0.5924 - val_loss: 2.7253 - val_accuracy: 0.3845\nEpoch 206/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3727 - accuracy: 0.5923 - val_loss: 2.7385 - val_accuracy: 0.3870\nEpoch 207/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3624 - accuracy: 0.5956 - val_loss: 2.7432 - val_accuracy: 0.3851\nEpoch 208/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3773 - accuracy: 0.5910 - val_loss: 2.7216 - val_accuracy: 0.3889\nEpoch 209/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3704 - accuracy: 0.5948 - val_loss: 2.7420 - val_accuracy: 0.3862\nEpoch 210/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3649 - accuracy: 0.5940 - val_loss: 2.7345 - val_accuracy: 0.3857\nEpoch 211/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3658 - accuracy: 0.5972 - val_loss: 2.7286 - val_accuracy: 0.3862\nEpoch 212/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3638 - accuracy: 0.5960 - val_loss: 2.7565 - val_accuracy: 0.3840\nEpoch 213/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3717 - accuracy: 0.5905 - val_loss: 2.7256 - val_accuracy: 0.3861\nEpoch 214/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3565 - accuracy: 0.5954 - val_loss: 2.7289 - val_accuracy: 0.3876\nEpoch 215/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3619 - accuracy: 0.5964 - val_loss: 2.7327 - val_accuracy: 0.3848\nEpoch 216/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3641 - accuracy: 0.5953 - val_loss: 2.7331 - val_accuracy: 0.3874\nEpoch 217/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3646 - accuracy: 0.5940 - val_loss: 2.7760 - val_accuracy: 0.3802\nEpoch 218/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3630 - accuracy: 0.5948 - val_loss: 2.7268 - val_accuracy: 0.3882\nEpoch 219/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3581 - accuracy: 0.5982 - val_loss: 2.7466 - val_accuracy: 0.3857\nEpoch 220/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3649 - accuracy: 0.5933 - val_loss: 2.7463 - val_accuracy: 0.3855\nEpoch 221/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3586 - accuracy: 0.5982 - val_loss: 2.7452 - val_accuracy: 0.3844\nEpoch 222/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3595 - accuracy: 0.5960 - val_loss: 2.7496 - val_accuracy: 0.3834\nEpoch 223/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3526 - accuracy: 0.5979 - val_loss: 2.7338 - val_accuracy: 0.3847\nEpoch 224/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3509 - accuracy: 0.5988 - val_loss: 2.7350 - val_accuracy: 0.3837\nEpoch 225/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3490 - accuracy: 0.5965 - val_loss: 2.7550 - val_accuracy: 0.3829\nEpoch 226/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3516 - accuracy: 0.5990 - val_loss: 2.7509 - val_accuracy: 0.3845\nEpoch 227/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3576 - accuracy: 0.5950 - val_loss: 2.7456 - val_accuracy: 0.3848\nEpoch 228/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3533 - accuracy: 0.5991 - val_loss: 2.7517 - val_accuracy: 0.3839\nEpoch 229/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3522 - accuracy: 0.5974 - val_loss: 2.7462 - val_accuracy: 0.3849\nEpoch 230/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3540 - accuracy: 0.5971 - val_loss: 2.7467 - val_accuracy: 0.3857\nEpoch 231/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3446 - accuracy: 0.6011 - val_loss: 2.7419 - val_accuracy: 0.3874\nEpoch 232/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3443 - accuracy: 0.5985 - val_loss: 2.7544 - val_accuracy: 0.3850\nEpoch 233/256\n391/391 [==============================] - 3s 9ms/step - loss: 1.3498 - accuracy: 0.5972 - val_loss: 2.7450 - val_accuracy: 0.3888\nEpoch 234/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3505 - accuracy: 0.5974 - val_loss: 2.7516 - val_accuracy: 0.3824\nEpoch 235/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3447 - accuracy: 0.5998 - val_loss: 2.7547 - val_accuracy: 0.3870\nEpoch 236/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3465 - accuracy: 0.5983 - val_loss: 2.7522 - val_accuracy: 0.3863\nEpoch 237/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3412 - accuracy: 0.5994 - val_loss: 2.7449 - val_accuracy: 0.3866\nEpoch 238/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3406 - accuracy: 0.6009 - val_loss: 2.7474 - val_accuracy: 0.3861\nEpoch 239/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3383 - accuracy: 0.6015 - val_loss: 2.7503 - val_accuracy: 0.3873\nEpoch 240/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3425 - accuracy: 0.6009 - val_loss: 2.7563 - val_accuracy: 0.3854\nEpoch 241/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3440 - accuracy: 0.5971 - val_loss: 2.7535 - val_accuracy: 0.3855\nEpoch 242/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3419 - accuracy: 0.5999 - val_loss: 2.7501 - val_accuracy: 0.3886\nEpoch 243/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3341 - accuracy: 0.6033 - val_loss: 2.7563 - val_accuracy: 0.3833\nEpoch 244/256\n391/391 [==============================] - 4s 9ms/step - loss: 1.3432 - accuracy: 0.6001 - val_loss: 2.7660 - val_accuracy: 0.3836\nEpoch 245/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3385 - accuracy: 0.6025 - val_loss: 2.7716 - val_accuracy: 0.3846\nEpoch 246/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3442 - accuracy: 0.5999 - val_loss: 2.7572 - val_accuracy: 0.3870\nEpoch 247/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3331 - accuracy: 0.6024 - val_loss: 2.7620 - val_accuracy: 0.3887\nEpoch 248/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3321 - accuracy: 0.6032 - val_loss: 2.7546 - val_accuracy: 0.3847\nEpoch 249/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3431 - accuracy: 0.6007 - val_loss: 2.7546 - val_accuracy: 0.3876\nEpoch 250/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3394 - accuracy: 0.6005 - val_loss: 2.7512 - val_accuracy: 0.3892\nEpoch 251/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3398 - accuracy: 0.5992 - val_loss: 2.7533 - val_accuracy: 0.3874\nEpoch 252/256\n391/391 [==============================] - 3s 8ms/step - loss: 1.3319 - accuracy: 0.6043 - val_loss: 2.7578 - val_accuracy: 0.3866\nEpoch 253/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3315 - accuracy: 0.6049 - val_loss: 2.7589 - val_accuracy: 0.3871\nEpoch 254/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3308 - accuracy: 0.6014 - val_loss: 2.7607 - val_accuracy: 0.3869\nEpoch 255/256\n391/391 [==============================] - 3s 9ms/step - loss: 1.3319 - accuracy: 0.6024 - val_loss: 2.7555 - val_accuracy: 0.3862\nEpoch 256/256\n391/391 [==============================] - 3s 7ms/step - loss: 1.3296 - accuracy: 0.6012 - val_loss: 2.7572 - val_accuracy: 0.3841\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f8e1b49bb90>"},"metadata":{}}]}]}